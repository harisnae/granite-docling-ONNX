# -*- coding: utf-8 -*-
"""IBM_Granite_docling_image_captioning_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tFuW7wvyqAVvpf8V1pOgJB-US30oFezL
"""

# Commented out IPython magic to ensure Python compatibility.
# --------------------------------------------------------------
# Cell‚ÄØ1Ô∏è‚É£ ‚Äì Install / upgrade all required packages (clean‚Äëup + install)
# --------------------------------------------------------------
# 1Ô∏è‚É£  Remove any old Optimum wheels (both the core package and the
#     separate `optimum‚Äëonnx` wrapper).  This eliminates the
#     ‚ÄúMultiple distributions found for package optimum‚Äù warning.
# 2Ô∏è‚É£  Install the **single, unified** Optimum wheel (‚â•‚ÄØ1.20) with the
#     ONNX‚ÄëRuntime extra.  The wheel already provides
#     `optimum.onnxruntime` and the class `ORTModelForVision2Seq`.
# 3Ô∏è‚É£  Install the remaining notebook dependencies.
# 4Ô∏è‚É£  Invalidate the import‚Äësystem caches and report the installed
#     versions so you can verify that everything is correct.
# --------------------------------------------------------------

# ------------------------------------------------------------------
# 1Ô∏è‚É£  Clean‚Äëup old Optimum installations (core + wrapper)
# ------------------------------------------------------------------
# The `-q` flag silences pip‚Äôs output; we also redirect stdout/
# stderr to /dev/null because the messages are not needed in the notebook.
!pip uninstall -y optimum optimum-onnx > /dev/null 2>&1

# ------------------------------------------------------------------
# 2Ô∏è‚É£  Install the unified Optimum package (with ONNX support)
# ------------------------------------------------------------------
# `>=1.20` guarantees that `ORTModelForVision2Seq` exists.
# %pip install -U "optimum[onnxruntime]>=1.20"

# ------------------------------------------------------------------
# 3Ô∏è‚É£  Install the remaining dependencies used by the notebook
# ------------------------------------------------------------------
# %pip install -q "transformers[onnx]" \
               huggingface_hub \
               pillow \
               torch   # let pip pick the latest compatible torch version

# ------------------------------------------------------------------
# 4Ô∏è‚É£  Refresh Python‚Äôs import‚Äësystem caches and show versions
# ------------------------------------------------------------------
import importlib, importlib.metadata

# Force the import machinery to re‚Äëscan the site‚Äëpackages directory.
importlib.invalidate_caches()

def _print_version(pkg_name: str, import_name: str = None):
    """Print the installed version of a package (metadata first, then __version__)."""
    import_name = import_name or pkg_name
    try:
        # Prefer the canonical metadata version ‚Äì works even if the module
        # does not expose a __version__ attribute.
        ver = importlib.metadata.version(import_name)
    except Exception:
        try:
            mod = importlib.import_module(import_name)
            ver = getattr(mod, "__version__", "unknown")
        except Exception as e:  # pragma: no cover
            ver = f"NOT INSTALLED ({e})"
    print(f"{pkg_name:<20} {ver}")

print("\nüîé Installed package versions:")
_print_version("optimum")                     # unified optimum package
_print_version("transformers")
_print_version("torch")
_print_version("huggingface_hub")
_print_version("pillow", "PIL")               # Pillow registers as `PIL`
_print_version("onnxruntime")

print("\n‚úÖ Packages installed (Optimum ‚â•‚ÄØ1.20).")
print("‚ö†Ô∏è  No runtime restart is needed **as long as** no earlier cell imported any of these packages.")

# --------------------------------------------------------------
# Cell‚ÄØ2Ô∏è‚É£ ‚Äì Import all Python modules we‚Äôll need later.
# --------------------------------------------------------------
import os                                 # File‚Äësystem utilities
from pathlib import Path                  # Convenient path handling
import numpy as np                       # Numerical arrays (used by ONNX‚ÄëRuntime)
from PIL import Image                    # Image loading / conversion (Pillow)

# Hugging‚ÄëFace utilities
from huggingface_hub import hf_hub_download, login

# Processor that knows how to build multimodal prompts
from transformers import AutoProcessor

# ------------------------------------------------------------------
# ONNX Runtime ‚Äì we will create three separate InferenceSession objects
# (vision encoder, token embedder, decoder) manually, as shown in the
# model‚Äëcard example.
# ------------------------------------------------------------------
import onnxruntime as ort

# ------------------------------------------------------------------
# Helper to load images (optional ‚Äì you can also use PIL directly)
# ------------------------------------------------------------------
from transformers.image_utils import load_image

# --------------------------------------------------------------
# Cell‚ÄØ3Ô∏è‚É£ ‚Äì Specify the Hugging‚ÄØFace repository that hosts the
#            ONNX‚Äëconverted Granite‚ÄëDocling model.
# --------------------------------------------------------------
# The repo contains three ONNX files under the `onnx/` sub‚Äëfolder.
model_id = "onnx-community/granite-docling-258M-ONNX"

print(f"‚úÖ Model repository set to: {model_id}")

# --------------------------------------------------------------
# Cell‚ÄØ4Ô∏è‚É£ ‚Äì Download the three ONNX model files to the local
#            Colab filesystem.
# --------------------------------------------------------------
# 1Ô∏è‚É£ vision_encoder.onnx   ‚Äì image encoder (SigLIP2)
# 2Ô∏è‚É£ embed_tokens.onnx     ‚Äì token embedding layer (Granite‚Äë165M)
# 3Ô∏è‚É£ decoder_model_merged.onnx ‚Äì text decoder (Idefics3‚Äëstyle)
# --------------------------------------------------------------

import getpass
from huggingface_hub import hf_hub_download, login

# ‚ë† request HF token (optional for private repos)
HF_TOKEN = getpass.getpass('üîë Enter your Hugging‚ÄØFace token (will not be echoed): ')
login          # optional but ensures auth for private repos

# ‚ë° download each ONNX file ‚Äì the .onnx_data companion is fetched automatically
vision_path = hf_hub_download(
    repo_id=model_id,
    filename="onnx/vision_encoder.onnx",
    token=HF_TOKEN,
)

embed_path = hf_hub_download(
    repo_id=model_id,
    filename="onnx/embed_tokens.onnx",
    token=HF_TOKEN,
)

decoder_path = hf_hub_download(
    repo_id=model_id,
    filename="onnx/decoder_model_merged.onnx",
    token=HF_TOKEN,
)

print("‚úÖ ONNX files downloaded:")
print(f"   Vision encoder  ‚Üí {vision_path}")
print(f"   Token embedder  ‚Üí {embed_path}")
print(f"   Decoder (LLM)  ‚Üí {decoder_path}")

# hf_hub_download will pull the companion‚ÄØ.onnx_data file automatically,
# but you can verify it and force a re‚Äëdownload if it‚Äôs missing:

from huggingface_hub import hf_hub_download, snapshot_download
import os, pathlib

def download_onnx(name):
    # download the .onnx file (this also fetches the .onnx_data)
    onnx_path = hf_hub_download(
        repo_id=model_id,
        filename=f"onnx/{name}.onnx",
        token=HF_TOKEN,
    )
    # ensure the matching .onnx_data file exists; if not, fetch the whole repo snapshot
    data_path = pathlib.Path(onnx_path).with_name(f"{name}.onnx_data")
    if not data_path.is_file():
        # fallback: download the entire repo (only .onnx/.onnx_data files)
        snapshot_download(
            repo_id=model_id,
            allow_patterns=[f"onnx/{name}.onnx", f"onnx/{name}.onnx_data"],
            token=HF_TOKEN,
        )
    return onnx_path

vision_path = download_onnx("vision_encoder")
embed_path  = download_onnx("embed_tokens")
decoder_path = download_onnx("decoder_model_merged")
print("‚úÖ All ONNX files and their .onnx_data weights are present.")

# --------------------------------------------------------------
# Cell‚ÄØ5Ô∏è‚É£ ‚Äì Load the Granite‚ÄëDocling model (manual ONNXRuntime sessions)
# --------------------------------------------------------------
def get_providers() -> list:
    """Return the optimal ONNX Runtime execution provider."""
    try:
        import torch
        if torch.cuda.is_available():
            return ["CUDAExecutionProvider", "CPUExecutionProvider"]
    except Exception:
        pass
    return ["CPUExecutionProvider"]

providers = get_providers()
import onnxruntime as ort
if providers[0] not in ort.get_available_providers():
    providers = ["CPUExecutionProvider"]
print(f"üîß ONNX Runtime providers: {providers}")

# --------------------------------------------------------------
# Processor (same as in the Space app)
# --------------------------------------------------------------
processor = AutoProcessor.from_pretrained(
    model_id,
    token=HF_TOKEN or None,
    trust_remote_code=True,
)

# --------------------------------------------------------------
# Load the model configuration (needed for KV‚Äëcache dimensions)
# --------------------------------------------------------------
from transformers import AutoConfig
config = AutoConfig.from_pretrained(model_id)

# Extract the fields required to build the past‚Äëkey‚Äëvalue cache.
# These attributes exist in the original PyTorch config; they are
# missing from the Idefics3Config, so we read them from the
# `text_config` sub‚Äëobject.
num_key_value_heads = config.text_config.num_key_value_heads
head_dim            = config.text_config.head_dim
num_hidden_layers   = config.text_config.num_hidden_layers
eos_token_id       = config.text_config.eos_token_id
image_token_id      = config.image_token_id

# --------------------------------------------------------------
# Download the three ONNX files (if not already cached)
# --------------------------------------------------------------
vision_path  = hf_hub_download(
    repo_id=model_id,
    subfolder="onnx",
    filename="vision_encoder.onnx",
)
embed_path   = hf_hub_download(
    repo_id=model_id,
    subfolder="onnx",
    filename="embed_tokens.onnx",
)
decoder_path = hf_hub_download(
    repo_id=model_id,
    subfolder="onnx",
    filename="decoder_model_merged.onnx",
)

# --------------------------------------------------------------
# Create ONNX Runtime inference sessions
# --------------------------------------------------------------
vision_session  = ort.InferenceSession(vision_path,  providers=[providers[0]])
embed_session   = ort.InferenceSession(embed_path,   providers=[providers[0]])
decoder_session = ort.InferenceSession(decoder_path, providers=[providers[0]])

print("‚úÖ ONNX Runtime sessions (vision, embed, decoder) created successfully.")

# --------------------------------------------------------------
# Cell‚ÄØ6Ô∏è‚É£ ‚Äì Vision ‚Üí embed ‚Üí decoder (image‚Äëconditioned caption)
# --------------------------------------------------------------

# --------------------------------------------------------------
# 0Ô∏è‚É£  Imports & optional Hugging‚ÄØFace token prompt
# --------------------------------------------------------------
import urllib.request, os
import numpy as np
from PIL import Image
import torchvision.transforms as T
from transformers import AutoTokenizer
import getpass
from huggingface_hub import login

HF_TOKEN = getpass.getpass('üîë Enter your Hugging‚ÄØFace token (leave empty to skip): ')
if HF_TOKEN:
    login(token=HF_TOKEN)
    os.environ["HF_TOKEN"] = HF_TOKEN

# --------------------------------------------------------------
# 1Ô∏è‚É£  ONNX Runtime sessions ‚Äì must already exist (created in Cell‚ÄØ5)
# --------------------------------------------------------------
#   vision_session, embed_session, decoder_session
#   num_hidden_layers, num_key_value_heads, head_dim
# If you run the notebook from the top, Cell‚ÄØ5 will have instantiated these.

# --------------------------------------------------------------
# 2Ô∏è‚É£  Helper ‚Äì display model I/O (useful for debugging)
# --------------------------------------------------------------
def show_inputs(sess, name: str):
    """Print the inputs of an ONNX Runtime session."""
    print(f"\n{name} inputs:")
    for i in sess.get_inputs():
        print(f"  ‚Ä¢ {i.name}  shape={i.shape}  type={i.type}")

show_inputs(vision_session,  "Vision encoder")
show_inputs(embed_session,   "Token embedder")
show_inputs(decoder_session, "Decoder")

# --------------------------------------------------------------
# 3Ô∏è‚É£  Image preprocessing ‚Äì 512√ó512, bool mask
# --------------------------------------------------------------
preprocess = T.Compose([
    T.Resize((512, 512)),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406],
                std =[0.229, 0.224, 0.225]),
])

def prepare_image(pil_img: Image.Image) -> dict:
    """
    Convert a PIL image into the dict expected by the vision encoder.
    Returns a dict with:
        - pixel_values: (1,1,3,512,512) float32
        - pixel_attention_mask: (1,1,512,512) bool
    """
    pixel_values = preprocess(pil_img).unsqueeze(0).numpy().astype(np.float32)  # (1,3,512,512)
    pixel_values = np.expand_dims(pixel_values, axis=1)                        # (1,1,3,512,512)
    mask = np.ones(pixel_values.shape[:2] + pixel_values.shape[3:], dtype=np.bool_)
    return {"pixel_values": pixel_values, "pixel_attention_mask": mask}

# --------------------------------------------------------------
# 4Ô∏è‚É£  Helper ‚Äì create an empty KV‚Äëcache for the first decoder step
# --------------------------------------------------------------
def empty_past(num_layers: int, batch: int, heads: int, head_dim: int) -> dict:
    """
    Build a dict with empty past_key_values tensors for each layer.
    Shape: (batch, heads, 0, head_dim) ‚Äì zero‚Äëlength sequence.
    """
    empty = np.empty((batch, heads, 0, head_dim), dtype=np.float32)
    past = {}
    for i in range(num_layers):
        past[f"past_key_values.{i}.key"]   = empty
        past[f"past_key_values.{i}.value"] = empty
    return past

# --------------------------------------------------------------
# 5Ô∏è‚É£  Generation option A ‚Äì Greedy (original) ‚Äì kept for reference
# --------------------------------------------------------------
def generate_caption_greedy(pil_img: Image.Image, max_len: int = 64) -> str:
    """Original greedy implementation (kept for comparison). Returns a space‚Äëseparated
    string of token IDs."""
    # Vision encoder -------------------------------------------------
    img_emb = vision_session.run(None, prepare_image(pil_img))[0]
    if img_emb.ndim == 4:                     # (1,1,1,embed_dim) ‚Üí (1,1,embed_dim)
        img_emb = np.squeeze(img_emb, axis=2)

    # BOS token ------------------------------------------------------
    bos_emb = embed_session.run(
        None,
        {"input_ids": np.array([[0]], dtype=np.int64)}
    )[0]

    # Initialise sequence + empty KV‚Äëcache ---------------------------
    seq_embeds = np.concatenate([img_emb, bos_emb], axis=1)   # (1,2,embed_dim)
    past = empty_past(num_hidden_layers, 1, num_key_value_heads, head_dim)

    tokenizer = AutoTokenizer.from_pretrained("onnx-community/granite-docling-258M-ONNX")
    eos_id = tokenizer.eos_token_id

    generated_ids = []

    for _ in range(max_len):
        attn_mask = np.ones((1, seq_embeds.shape[1]), dtype=np.int64)

        out = decoder_session.run(
            None,
            {"inputs_embeds": seq_embeds,
             "attention_mask": attn_mask,
             **past},
        )
        logits = out[0][0, -1, :]                     # (vocab,)

        next_id = int(np.argmax(logits))
        generated_ids.append(next_id)

        if next_id == eos_id:
            break

        # Append new token embedding
        next_emb = embed_session.run(
            None,
            {"input_ids": np.array([[next_id]], dtype=np.int64)},
        )[0]
        seq_embeds = np.concatenate([seq_embeds, next_emb], axis=1)

        # Re‚Äëbuild KV‚Äëcache
        past = {}
        for i in range(num_hidden_layers):
            past[f"past_key_values.{i}.key"]   = out[1 + i * 2]
            past[f"past_key_values.{i}.value"] = out[2 + i * 2]

    return " ".join(map(str, generated_ids))

# --------------------------------------------------------------
# 6Ô∏è‚É£  Generation option B ‚Äì Sampling (top‚Äëk / nucleus)
# --------------------------------------------------------------
def generate_caption_sampling(pil_img: Image.Image,
                              max_len: int = 64,
                              top_k: int = 50,
                              top_p: float = 0.9) -> str:
    """Stochastic decoding using top‚Äëk and/or nucleus (top‚Äëp) sampling.
    Returns a space‚Äëseparated string of token IDs."""
    # Vision encoder -------------------------------------------------
    img_emb = vision_session.run(None, prepare_image(pil_img))[0]
    if img_emb.ndim == 4:
        img_emb = np.squeeze(img_emb, axis=2)

    # BOS token ------------------------------------------------------
    bos_emb = embed_session.run(
        None,
        {"input_ids": np.array([[0]], dtype=np.int64)}
    )[0]

    # Initialise sequence + empty KV‚Äëcache ---------------------------
    seq_embeds = np.concatenate([img_emb, bos_emb], axis=1)
    past = empty_past(num_hidden_layers, 1, num_key_value_heads, head_dim)

    tokenizer = AutoTokenizer.from_pretrained("onnx-community/granite-docling-258M-ONNX")
    eos_id = tokenizer.eos_token_id

    generated_ids = []

    for _ in range(max_len):
        attn_mask = np.ones((1, seq_embeds.shape[1]), dtype=np.int64)

        out = decoder_session.run(
            None,
            {"inputs_embeds": seq_embeds,
             "attention_mask": attn_mask,
             **past},
        )
        logits = out[0][0, -1, :]                     # (vocab,)

        # ---------- top‚Äëk filtering ----------
        if top_k > 0:
            kth_vals = np.partition(logits, -top_k)[-top_k:]
            kth_min = kth_vals.min()
            logits = np.where(logits >= kth_min, logits, -np.inf)

        # ---------- nucleus (top‚Äëp) filtering ----------
        sorted_idx = np.argsort(-logits)               # descending order
        sorted_logits = logits[sorted_idx]
        probs = np.exp(sorted_logits - np.max(sorted_logits))
        probs /= probs.sum()
        cumulative = np.cumsum(probs)
        cutoff = cumulative > top_p
        if cutoff.any():
            cutoff_idx = np.argmax(cutoff)
            keep_idx = sorted_idx[: cutoff_idx + 1]
            probs = probs[: cutoff_idx + 1]
            probs /= probs.sum()
        else:
            keep_idx = sorted_idx

        # ---------- sample ----------
        next_id = int(np.random.choice(keep_idx, p=probs))
        generated_ids.append(next_id)

        if next_id == eos_id:
            break

        # Append embedding of the sampled token
        next_emb = embed_session.run(
            None,
            {"input_ids": np.array([[next_id]], dtype=np.int64)},
        )[0]
        seq_embeds = np.concatenate([seq_embeds, next_emb], axis=1)

        # Update KV‚Äëcache
        past = {}
        for i in range(num_hidden_layers):
            past[f"past_key_values.{i}.key"]   = out[1 + i * 2]
            past[f"past_key_values.{i}.value"] = out[2 + i * 2]

    return " ".join(map(str, generated_ids))

# --------------------------------------------------------------
# 7Ô∏è‚É£  Generation option C ‚Äì Full‚Äësequence **without** KV‚Äëcache
# --------------------------------------------------------------
def generate_caption_no_cache(pil_img: Image.Image, max_len: int = 64) -> str:
    """Runs the decoder without ever feeding a KV‚Äëcache.  Because the ONNX
    model still declares the past‚Äëkey‚Äëvalue inputs as required, we provide
    *empty* tensors for them on every step (zero‚Äëlength sequence).  This
    satisfies the runtime while keeping the semantics of ‚Äúno cache‚Äù
    (the decoder recomputes its internal cache each step)."""

    # Vision encoder -------------------------------------------------
    img_emb = vision_session.run(None, prepare_image(pil_img))[0]
    if img_emb.ndim == 4:                     # (1,1,1,embed_dim) ‚Üí (1,1,embed_dim)
        img_emb = np.squeeze(img_emb, axis=2)

    # BOS token ------------------------------------------------------
    bos_emb = embed_session.run(
        None,
        {"input_ids": np.array([[0]], dtype=np.int64)}
    )[0]

    # Initialise sequence ---------------------------------------------
    seq_embeds = np.concatenate([img_emb, bos_emb], axis=1)

    tokenizer = AutoTokenizer.from_pretrained("onnx-community/granite-docling-258M-ONNX")
    eos_id = tokenizer.eos_token_id

    generated_ids = []

    for _ in range(max_len):
        attn_mask = np.ones((1, seq_embeds.shape[1]), dtype=np.int64)

        # Provide empty past‚Äëkey‚Äëvalues (required by the model)
        past = empty_past(num_hidden_layers, 1, num_key_value_heads, head_dim)

        out = decoder_session.run(
            None,
            {"inputs_embeds": seq_embeds,
             "attention_mask": attn_mask,
             **past},
        )
        logits = out[0][0, -1, :]                     # (vocab,)

        next_id = int(np.argmax(logits))
        generated_ids.append(next_id)

        if next_id == eos_id:
            break

        # Append embedding of the newly generated token
        next_emb = embed_session.run(
            None,
            {"input_ids": np.array([[next_id]], dtype=np.int64)},
        )[0]
        seq_embeds = np.concatenate([seq_embeds, next_emb], axis=1)

    return " ".join(map(str, generated_ids))

# --------------------------------------------------------------
# 8Ô∏è‚É£  Generation option D ‚Äì Correct BOS/EOS IDs from tokenizer
# --------------------------------------------------------------
def generate_caption_correct_ids(pil_img: Image.Image, max_len: int = 64) -> str:
    """Retrieves BOS and EOS token IDs from the tokenizer (instead of hard‚Äëcoding).
    Uses KV‚Äëcache + full‚Äësequence inputs (same pattern as the original greedy)."""
    tokenizer = AutoTokenizer.from_pretrained("onnx-community/granite-docling-258M-ONNX")
    bos_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 0
    eos_id = tokenizer.eos_token_id

    # Vision encoder -------------------------------------------------
    img_emb = vision_session.run(None, prepare_image(pil_img))[0]
    if img_emb.ndim == 4:
        img_emb = np.squeeze(img_emb, axis=2)

    # BOS embedding ---------------------------------------------------
    bos_emb = embed_session.run(
        None,
        {"input_ids": np.array([[bos_id]], dtype=np.int64)},
    )[0]

    # Initialise sequence + empty KV‚Äëcache ---------------------------
    seq_embeds = np.concatenate([img_emb, bos_emb], axis=1)
    past = empty_past(num_hidden_layers, 1, num_key_value_heads, head_dim)

    generated_ids = []

    for _ in range(max_len):
        attn_mask = np.ones((1, seq_embeds.shape[1]), dtype=np.int64)

        out = decoder_session.run(
            None,
            {"inputs_embeds": seq_embeds,
             "attention_mask": attn_mask,
             **past},
        )
        logits = out[0][0, -1, :]                     # (vocab,)

        next_id = int(np.argmax(logits))
        generated_ids.append(next_id)

        if next_id == eos_id:
            break

        # Append new token embedding
        next_emb = embed_session.run(
            None,
            {"input_ids": np.array([[next_id]], dtype=np.int64)},
        )[0]
        seq_embeds = np.concatenate([seq_embeds, next_emb], axis=1)

        # Update KV‚Äëcache
        past = {}
        for i in range(num_hidden_layers):
            past[f"past_key_values.{i}.key"]   = out[1 + i * 2]
            past[f"past_key_values.{i}.value"] = out[2 + i * 2]

    return " ".join(map(str, generated_ids))

# --------------------------------------------------------------
# 9Ô∏è‚É£  Helper ‚Äì decode token IDs to readable text
# --------------------------------------------------------------
def decode_ids(token_ids_str: str) -> str:
    """Convert a space‚Äëseparated string of token IDs into a human‚Äëreadable caption."""
    tokenizer = AutoTokenizer.from_pretrained("onnx-community/granite-docling-258M-ONNX")
    ids = [int(t) for t in token_ids_str.split()]
    return tokenizer.decode(ids, skip_special_tokens=True)

# --------------------------------------------------------------
# üîü  Load a sample image (you can replace the URL with any image)
# --------------------------------------------------------------
img_url = (
    "https://huggingface.co/spaces/ibm-granite/granite-docling-258m-demo/resolve/main/data/images/"
    "lake-zurich-switzerland-view-nature-landscapes-7bbda4-1024.jpg"
)
img_path = "/tmp/sample.jpg"
urllib.request.urlretrieve(img_url, img_path)
sample_image = Image.open(img_path).convert("RGB")

# --------------------------------------------------------------
# 1Ô∏è‚É£1Ô∏è‚É£  Run each generation variant and print results
# --------------------------------------------------------------
print("\n=== Greedy (original) ===")
ids_greedy = generate_caption_greedy(sample_image)
print("Token IDs :", ids_greedy)
print("Caption   :", decode_ids(ids_greedy))

print("\n=== Sampling (top‚Äëk/‚Äãtop‚Äëp) ===")
ids_sampling = generate_caption_sampling(sample_image, top_k=50, top_p=0.9)
print("Token IDs :", ids_sampling)
print("Caption   :", decode_ids(ids_sampling))

print("\n=== No‚Äëcache (full‚Äësequence only) ===")
ids_no_cache = generate_caption_no_cache(sample_image)
print("Token IDs :", ids_no_cache)
print("Caption   :", decode_ids(ids_no_cache))

print("\n=== Correct BOS/EOS IDs ===")
ids_correct_ids = generate_caption_correct_ids(sample_image)
print("Token IDs :", ids_correct_ids)
print("Caption   :", decode_ids(ids_correct_ids))